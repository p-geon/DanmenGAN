{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"for_GitHub.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XCwbOhGepL9Q","colab_type":"text"},"source":["# Danmen-GAN\n","\n","断面二次モーメントを上げ下げできるヤツ\n","\n","<img src=\"https://raw.githubusercontent.com/p-geon/DanmenGAN/master/Gainen.png\"></img>\n"]},{"cell_type":"markdown","metadata":{"id":"aIw9MWiwlkdf","colab_type":"text"},"source":["# 基本設定"]},{"cell_type":"code","metadata":{"id":"AEYqmB8UhAbI","colab_type":"code","colab":{}},"source":["# Mount data\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","%cd /content/gdrive/My\\ Drive/GAN/exp\n","\n","! pip install tensorflow==2.2\n","! pip install tensorflow_addons==0.6.0 --no-deps\n","\n","import sys # バージョン確認用\n","import datetime # パラメータと画像保存用\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from skimage.io import imsave\n","from skimage import img_as_ubyte # 警告回避用\n","\n","print(tf.__version__)\n","\n","! pip install StealthFlow==0.0.13\n","from stealthflow.fid import FIDNumpy, FIDTF"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qGv_QK66kZBE","colab_type":"text"},"source":["# 断面二次モーメントを求めるグラフ\n","\n","- `calc_second_moment_of_area`: 画像 `4-D Tensor` を入力とし、`I_x`, `I_y`, `I_r` (`1-D Tensor`x3)を出力するグラフ\n","- `calc_second_moment_of_area_for_generated_images`: 外側からこの関数を TensorFlow の関数として使えるようにした"]},{"cell_type":"code","metadata":{"id":"_91xexMhhLJM","colab_type":"code","colab":{}},"source":["class SecondMomentOfArea:\n","    def __init__(self, img_shape=(28, 28)):\n","        \"\"\"\n","        1. ピクセルの中心が重心, 0.5^27.5\n","        2. (全て1の場合) 画像の中心(14, 14)が重心\n","        となるようにする\n","        \"\"\"\n","        # [0, 1, 2, ..., 27]\n","        arange_x = 0.5 + np.arange(0, img_shape[0], 1) # 縦のピクセル数\n","        arange_y = 0.5 + np.arange(0, img_shape[1], 1) # 横のピクセル数\n","\n","        # ピクセルの中心が重心, 0.5^27.5\n","        distance_vector_x = np.asarray([0.5+d for d in range(img_shape[1])])\n","        distance_matrix_x = np.tile(distance_vector_x, (img_shape[0], 1)) # xからの距離 (MNISTの端 → 27.5)\n","        distance_matrix_y = distance_matrix_x.T # yからの距離\n","\n","        \"\"\"\n","        正規化用マトリックス(この設定における最大 = 全ピクセル使用)\n","        \"\"\"\n","        # 縦方向(y)に対する断面二次モーメント(I_x)を正規化するため、最大の断面二次モーメントを求める\n","        matrix_for_norm_I_x = np.tile(np.abs(arange_y - img_shape[0]/2.0), (img_shape[1], 1)).T\n","        norm_I_x = np.sum(matrix_for_norm_I_x)\n","\n","        # 横方向(x), I_y\n","        matrix_for_norm_I_y = np.tile(np.abs(arange_x - img_shape[1]/2.0), (img_shape[0], 1)).T\n","        norm_I_y = np.sum(matrix_for_norm_I_y)\n","         \n","        \"\"\"\n","        to TFconstant\n","        \"\"\"\n","        self.arange_x = tf.constant(arange_x, dtype=tf.float32) # (28, )\n","        self.arange_y = tf.constant(arange_y, dtype=tf.float32) # (28,)\n","        self.distance_matrix_x = tf.constant(distance_matrix_x[np.newaxis, :, :, np.newaxis], dtype=tf.float32) # (1, 28, 28, 1)\n","        self.distance_matrix_y = tf.constant(distance_matrix_y[np.newaxis, :, :, np.newaxis], dtype=tf.float32) #(1, 28, 28, 1)\n","        self.norm_I_x = tf.constant(norm_I_x, dtype=tf.float32) #()\n","        self.norm_I_y = tf.constant(norm_I_y, dtype=tf.float32) #()\n","\n","    def calc_second_moment_of_area(self, img): # (None, 28, 28, 1)\n","        \"\"\"\n","        断面二次モーメントの計算\n","        \"\"\"\n","\n","        \"\"\"\n","        中立軸の計算\n","        \"\"\"\n","        # 密度。ゼロじゃない画素の割合　\n","        density = (tf.reduce_sum(img, axis=[1, 2], keepdims=True)/(img.shape[1]*img.shape[2]))\n","        # (1, 28, 28, 1) x (None, 28, 28, 1) -> (None, 28, 28, 1)\n","        x_moment = tf.math.divide_no_nan(tf.math.multiply(self.distance_matrix_x, img), density) # ゼロ除算回避付\n","        y_moment = tf.math.divide_no_nan(tf.math.multiply(self.distance_matrix_y, img), density)\n","\n","        # (None, 28, 28, 1) -> (None, )\n","        neutral_axis_x = tf.math.reduce_mean(x_moment, axis=[1, 2])\n","        neutral_axis_y = tf.math.reduce_mean(y_moment, axis=[1, 2])\n","\n","        \"\"\"\n","        断面二次モーメント (縦)\n","        I_x = ∫_A y^2 dA\n","        \"\"\"\n","        # sub: (None, 28, ) - (None, ) -> abs: (None, 28)\n","        dy = tf.math.abs(self.arange_y - neutral_axis_y)\n","        # (None, 28) -> (None, 1, 28)\n","        dy = tf.reshape(dy, shape=[-1, img.shape[1], 1])\n","        # (None, 1, 28) -> (None, 28, 28)\n","        matrix_x = tf.tile(dy, multiples=[1, 1, img.shape[2]])\n","        # (None, 28, 28) -> (None, 28, 28, 1)\n","        matrix_x = tf.expand_dims(matrix_x, 3)\n","        # (None, 28, 28, 1)x(None, 28, 28, 1) -> (None, 28, 28, 1) -> (None,)\n","        I_x = tf.math.reduce_sum(tf.math.multiply(matrix_x, img), axis=[1, 2])/self.norm_I_x\n","\n","        \"\"\"\n","        断面二次モーメント (横)\n","        I_y = ∫_A x^2 dA\n","        \"\"\"\n","        # sub: (None, 28, ) - (None, ) -> abs: (None, 28)\n","        dx = tf.math.abs(self.arange_x - neutral_axis_x)\n","        # (None, 28) -> (None, 28, 1)\n","        dx = tf.reshape(dx, shape=[-1, 1, img.shape[2]])\n","        # (None, 1, 28) -> (None, 28, 28)\n","        matrix_y = tf.tile(dx, multiples=[1, img.shape[1], 1])\n","        # (None, 28, 28) -> (None, 28, 28, 1)\n","        matrix_y = tf.expand_dims(matrix_y, 3)\n","        # (None, 28, 28, 1)x(None, 28, 28, 1) -> (None, 28, 28, 1) -> (None,)\n","        I_y = tf.math.reduce_sum(tf.math.multiply(matrix_y, img), axis=[1, 2])/self.norm_I_y\n","        \"\"\"\n","        断面二次極モーメント (正規化のため 2.0 で割る)\n","        \"\"\"\n","        I_r = (I_x + I_y)/2.0\n","\n","        I_x = tf.keras.layers.Lambda(lambda x: x, name=\"I_x\")(I_x) # 視認性を上げるための恒等関数\n","        I_y = tf.keras.layers.Lambda(lambda x: x, name=\"I_y\")(I_y) # 視認性を上げるための恒等関数\n","        I_r = tf.keras.layers.Lambda(lambda x: x, name=\"I_r\")(I_r) # 視認性を上げるための恒等関数\n","\n","        return I_x, I_y, I_r\n","\n","    @tf.function\n","    def calc_second_moment_of_area_for_generated_images(self, img):\n","        return self.calc_second_moment_of_area(img)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfBDxgWXpic8","colab_type":"text"},"source":["# GAN\n","\n","Generator, Discirminator, Combined の三つで構成されている。"]},{"cell_type":"code","metadata":{"id":"pS165a_0phWx","colab_type":"code","colab":{}},"source":["def build_generator(params, smoa):\n","    # Noise\n","    z = z_in = tf.keras.layers.Input(shape=(params.NOISE_DIM, ), name=\"noise\")\n","\n","    # (NOISE_DIM, ) -> (1024, )\n","    x = tf.keras.layers.Dense(1024)(z)\n","    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n","    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\n","\n","    # (1024, ) -> (7*7*64, ) -> (7, 7, 64)\n","    x = tf.keras.layers.Dense(7*7*64)(z)\n","    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n","    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\n","    x = tf.keras.layers.Reshape(target_shape=(7, 7, 64))(x)\n","\n","    # (7, 7, 64) -> (14, 14, 32)\n","    x = tf.keras.layers.Conv2DTranspose(32, kernel_size=(5, 5)\n","        , padding='same', strides=(2, 2), use_bias=False, activation=None)(x)\n","    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)\n","    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n","\n","    # (14, 14, 128) -> (28, 28, 1)\n","    x = tf.keras.layers.Conv2DTranspose(1, kernel_size=(5, 5)\n","        , padding='same', strides=(2, 2), use_bias=False, activation=None)(x)\n","    img = tf.math.tanh(x)\n","    y = tf.keras.layers.Lambda(lambda x: x, name=\"generated_image\")(img) # img は後ろで使うので y に変数名を変更しておく\n","\n","    \"\"\"\n","    断面二次モーメントの計算 (ResNet みたいなグラフになる)\n","        線画薄い場所＝面積が減っている とする。\n","        実際のプリントとは異なってしまうが、今回は仕方ない。\n","    \"\"\"\n","    # range: [-1.0, 1.0] -> [0.0, 1.0]\n","    img = (img + 1.0)/2.0\n","    I_x, I_y, I_r = smoa.calc_second_moment_of_area(img)\n","\n","    return tf.keras.Model(inputs=z_in, outputs=[y, I_x, I_y, I_r])\n","\n","def build_discriminator():\n","    # real or generated\n","    x = x_in = tf.keras.layers.Input(shape=(28, 28, 1))\n","\n","    # (28, 28, 1) -> (14, 14, 32)\n","    x = tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(2, 2), padding='same')(x)\n","    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n","\n","    # (14, 14, 32) -> (7, 7, 64)\n","    x = tf.keras.layers.Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same')(x)\n","    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n","\n","    # (7, 7, 64) -> (7*7*64, ) -> (1024, )\n","    x = tf.keras.layers.Reshape(target_shape=(7*7*64, ))(x)\n","    x = tf.keras.layers.Dense(1024)(x)\n","    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n","\n","    # (1024, ) -> (1, ), range:[0, 1]\n","    x = tf.keras.layers.Dense(1)(x)\n","    p = tf.math.sigmoid(x)\n","\n","    return tf.keras.Model(inputs=x_in, outputs=p)\n","    \n","#-----------------------------------------------------------------------------------------------------------------------------\n","\n","class GAN:\n","    def __init__(self, params, smoa):\n","        self.params = params\n","        self.smoa = smoa\n","        self.build_model()\n","\n","    def build_model(self):\n","        self.discriminator = build_discriminator()\n","        self.discriminator.compile(\n","              loss = self.params.discriminator_loss\n","            , optimizer = self.params.discriminator_optimizer\n","            , metrics =  self.params.discriminator_metrics\n","            )\n","        self.discriminator.trainable=False\n","\n","        self.generator = build_generator(self.params, self.smoa)\n","        self.combined = self.build_combined()\n","        self.combined.compile(\n","              loss = self.params.generator_loss\n","            , optimizer = self.params.generator_optimizer\n","            , metrics = self.params.generator_metrics\n","            , loss_weights = self.params.generator_loss_weights\n","            )\n","      \n","    def build_combined(self):\n","        z_in = tf.keras.layers.Input(shape=(self.params.NOISE_DIM, ))\n","\n","        y, I_x, I_y, I_r = self.generator(z_in)\n","\n","        y = tf.keras.layers.Lambda(lambda x: x, name=\"generated_image\")(y)\n","        I_x = tf.keras.layers.Lambda(lambda x: x, name=\"I_x\")(I_x) # ロス用の名前付けレイヤー(恒等関数)\n","        I_y = tf.keras.layers.Lambda(lambda x: x, name=\"I_y\")(I_y) # ロス用の名前付けレイヤー(恒等関数)\n","        I_r = tf.keras.layers.Lambda(lambda x: x, name=\"I_r\")(I_r) # ロス用の名前付けレイヤー(恒等関数)\n","\n","        p = self.discriminator(y)\n","\n","        p = tf.keras.layers.Lambda(lambda x: x, name=\"possibility\")(p) # ロス用の名前付けレイヤー(恒等関数)\n","\n","        model = tf.keras.Model(inputs=z_in, outputs=[p, I_x, I_y, I_r])\n","        return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8IH81rWrceJ","colab_type":"text"},"source":["# Custom Metrics: Second Moment of Area\n","\n","断面二次モーメントを、`tf.keras` の評価時に出力できるようにしたもの。"]},{"cell_type":"code","metadata":{"id":"5j7JCRUvrb56","colab_type":"code","colab":{}},"source":["class MetricsAverageSecondMomentOfArea(tf.keras.metrics.Metric):\n","    def __init__(self, name=\"average_second_moment_of_area\", **kwargs):\n","        super(MetricsAverageSecondMomentOfArea, self).__init__(name=name, **kwargs)\n","        self.average_smoa = self.add_weight(name=\"average_second_moment_of_area\", initializer=\"zeros\")\n","    \n","    def update_state(self, y_true, y_pred):\n","        #y_true = tf.cast(y_true, tf.float32) # 断面二次モーメントの合計を求めるだけなので使わない\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        self.average_smoa.assign_add(tf.reduce_mean(y_pred))\n","\n","    def result(self):\n","        return self.average_smoa"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3B5xahtvHa_m","colab_type":"text"},"source":["# Custom Metrics: Frechet Inception Distance\n","\n","一行で FID を計算するツールを作ったので、それを使った。\n","\n","詳しくは https://note.com/hyper_pigeon/n/n9c5643413cd7 に書いてある。便利。"]},{"cell_type":"code","metadata":{"id":"I1gnIMEEHfG9","colab_type":"code","colab":{}},"source":["calc_fid = FIDNumpy(batch_size=50, scaling=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cyAMi-Bxxz8E","colab_type":"text"},"source":["# generate samples / calc FID\n","\n","- `generate_samples`: 結果確認用の画像生成とその保存、ついでにその断面二次モーメントも計算して結果の確認。`calc_second_moment_of_area_for_generated_images` はここで叩く。\n","\n","- `calc_fid_for_generator`: FID の計算。とにかく遅い。\n"]},{"cell_type":"code","metadata":{"id":"1CiUI2TbxzhB","colab_type":"code","colab":{}},"source":["def generate_samples(params, GAN, iteration):\n","    \"\"\"\n","    [img1, img2, img3] みたいなバッチを\n","\n","    [img1, img2,\n","    img3, 0] みたいな画像にする\n","    \"\"\"\n","    # Predict\n","    batch, I_x, I_y, I_r = GAN.generator.predict(params.FIXED_NOISE_FOR_PREDICT, batch_size=params.BATCH_SIZE)\n","    # range: [-1.0, 1.0] -> [0.0, 1.0]\n","    batch = batch / 2.0 + 0.5\n","\n","    batch_tensor = tf.convert_to_tensor(batch)\n","    I_x, I_y, I_r = GAN.smoa.calc_second_moment_of_area_for_generated_images(batch_tensor) # ここで smoa.calc_second_moment_of_area を使い回す\n","    print(f\"I_x:{I_x.numpy().mean():.3f}, I_y:{I_y.numpy().mean():.3f}, I_r:{I_r.numpy().mean():.3f}\")\n","\n","    length = int(np.sqrt(batch.shape[0]))+1\n","    width, height = length*batch.shape[1], length*batch.shape[2]\n","    img_buffer = np.zeros(shape=[width, height], dtype=np.float32)\n","\n","    for h in range(length):\n","        for w in range(length):\n","            if(h*length+w >= batch.shape[0]):break\n","            img_buffer[h*batch.shape[2]:h*batch.shape[2]+batch.shape[2], w*batch.shape[1]:w*batch.shape[1]+batch.shape[1]] = batch[h*length+w, :, :, 0]\n","\n","    # Output\n","    plt.imshow(img_buffer, cmap = \"gray\", vmin=0.0, vmax=1.0)\n","    plt.show()\n","    # Save\n","    imsave(fname=f\"_{params.EXPERIMENTAL_NAME}_{iteration}_image.png\", arr=img_as_ubyte(img_buffer))\n","\n","def calc_fid_for_generator(params, GAN, MNIST):\n","    import time\n","    start_time = time.time()\n","    NUMDATA_FID = 5000\n","\n","    orig = (MNIST.train_X[:NUMDATA_FID] / 2.0 + 0.5).astype(np.float32)\n","\n","    print(orig.shape, orig.dtype)\n","    gens = np.zeros(shape=[NUMDATA_FID, 28, 28, 1])\n","\n","    for i in range(NUMDATA_FID//params.BATCH_SIZE):\n","\n","        NOISE = np.random.normal(0, 1, (params.BATCH_SIZE, params.NOISE_DIM))\n","        # Predict\n","        batch, I_x, I_y, I_r = GAN.generator.predict(NOISE, batch_size=params.BATCH_SIZE)\n","        # range: [-1.0, 1.0] -> [0.0, 1.0]\n","        batch = batch / 2.0 + 0.5\n","        gens[params.BATCH_SIZE*i:params.BATCH_SIZE*(i+1), :, :, :] = batch\n","\n","    fid = FIDNumpy(batch_size=params.BATCH_SIZE, scaling=True)(gens, orig)\n","    print(\"FID\", fid)\n","    print(f\"Spent time: {time.time()-start_time}[s]\")\n","    return fid"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKdwNacxES6b","colab_type":"text"},"source":["# Loss Utils\n","\n","ロスなどを毎イテレーションおきに追加して、一定のスパンでその値の平均と標準偏差を求め、グラフ用にする。FID を毎イテレーション計算するのは時間がかかりすぎるので、グラフ作成時のみ取得する。\n","\n","全然いい書き方が思いつかない。"]},{"cell_type":"code","metadata":{"id":"m2lChUPnESBN","colab_type":"code","colab":{}},"source":["class Loss:\n","    def __init__(self, params):\n","\n","        self.iteration = []\n","        self.__initialize_losses()\n","        self.FID_list = []\n","\n","        self.capsize = 7\n","        self.markersize = 7\n","        self.alpha = 0.5\n","        self.params = params\n","    \n","    def dump_FID(self, FID):\n","        self.FID_list.append(FID)\n","\n","    def dump_loss_dicts(self, D_loss_real_dict, D_loss_fake_dict, G_loss_dict):\n","\n","        self.acc_D_real.append(D_loss_real_dict[\"accuracy\"])\n","        self.acc_D_fake.append(D_loss_fake_dict[\"accuracy\"])\n","        \n","        self.loss_D_real.append(D_loss_real_dict[\"loss\"])\n","        self.loss_D_fake.append(D_loss_fake_dict[\"loss\"])\n","        self.loss_G_fake.append(G_loss_dict[\"possibility_loss\"])\n","\n","        self.MSE_I_x.append(G_loss_dict[\"I_x_mean_squared_error\"])\n","        self.MSE_I_y.append(G_loss_dict[\"I_y_mean_squared_error\"])\n","        self.MSE_I_r.append(G_loss_dict[\"I_r_mean_squared_error\"])\n","\n","        self.I_x.append(G_loss_dict[\"I_x_average_second_moment_of_area\"])\n","        self.I_y.append(G_loss_dict[\"I_y_average_second_moment_of_area\"])\n","        self.I_r.append(G_loss_dict[\"I_r_average_second_moment_of_area\"])\n","        \n","    def show_loss(self, iteration):\n","\n","        self.iteration.append(iteration)\n","        self.__summarize_losses()\n","\n","        fig = plt.figure(figsize=(8,16), dpi=108)\n","\n","        # GAN Accuracy\n","        ax = fig.add_subplot(6, 1, 1)\n","        ax = self.__arrange_graph(ax)\n","        ax.set_ylim([0.0, 100.0])\n","        ax.set_ylabel(\"%\")\n","        ax.errorbar(self.iteration, 100.0*np.asarray(self.mean_acc_D_real), yerr=100.0*np.asarray(self.std_acc_D_real)\n","            , capsize=self.capsize, fmt='.-', markersize=self.markersize, ecolor='teal', markeredgecolor = \"teal\", color='teal', alpha=self.alpha, label=\"Accuracy:D_real_to_real\")\n","        ax.errorbar(self.iteration, 100.0*np.asarray(self.mean_acc_D_fake), yerr=100.0*np.asarray(self.std_acc_D_fake)\n","            , capsize=self.capsize, fmt='.:', markersize=self.markersize, ecolor='lightcoral', markeredgecolor = \"lightcoral\", color='lightcoral', alpha=self.alpha, label=\"Accuracy:D_fake_to_fake\")\n","        ax.legend()    \n","\n","        # GAN loss\n","        ax = fig.add_subplot(6, 1, 2)\n","        ax = self.__arrange_graph(ax)\n","        ax.set_ylim([-5.0, 5.0])\n","        ax.set_ylabel(\"log-loss\")\n","        ax.errorbar(self.iteration, np.log(self.mean_loss_D_real), yerr=np.log(self.std_loss_D_real)\n","            , capsize=self.capsize, fmt='.-', markersize=self.markersize, ecolor='teal', markeredgecolor = \"teal\", color='teal', alpha=self.alpha, label=\"loss:D_real_to_real\")\n","        ax.errorbar(self.iteration, np.log(self.mean_loss_D_fake), yerr=np.log(self.std_loss_D_fake)\n","            , capsize=self.capsize, fmt='.:', markersize=self.markersize, ecolor='lightcoral', markeredgecolor = \"lightcoral\", color='lightcoral', alpha=self.alpha, label=\"loss:D_fake_to_fake\")\n","        ax.errorbar(self.iteration, np.log(self.mean_loss_G_fake), yerr=np.log(self.std_loss_G_fake)\n","            , capsize=self.capsize, fmt='.-.', markersize=self.markersize, ecolor='orangered', markeredgecolor = \"orangered\", color='orangered', alpha=self.alpha, label=\"loss:G_fake_to_real\")\n","        \n","        ax.legend()    \n","\n","        # MSE loss\n","        ax = fig.add_subplot(6, 1, 3)\n","        ax = self.__arrange_graph(ax)\n","        ax.set_ylim([0.0, 1.0])\n","        ax.set_ylabel(\"MSE\")\n","        ax.errorbar(self.iteration, self.mean_MSE_I_x, yerr=self.std_MSE_I_x\n","            , capsize=self.capsize, fmt='.-', markersize=self.markersize, ecolor='steelblue', markeredgecolor = \"steelblue\", color='steelblue', alpha=self.alpha, label=\"MSE:I_x\")\n","        ax.errorbar(self.iteration, self.mean_MSE_I_y, yerr=self.std_MSE_I_y\n","            , capsize=self.capsize, fmt='.:', markersize=self.markersize, ecolor='salmon', markeredgecolor = \"salmon\", color='salmon', alpha=self.alpha, label=\"MSE:I_y\")\n","        ax.errorbar(self.iteration, self.mean_MSE_I_r, yerr=self.std_MSE_I_r\n","            , capsize=self.capsize, fmt='.-.', markersize=self.markersize, ecolor='dimgray', markeredgecolor = \"dimgray\", color='dimgray', alpha=self.alpha, label=\"MSE:I_r\")\n","        ax.legend()\n","\n","        # I_x, I_y, I_r\n","        ax = fig.add_subplot(6, 1, 4)\n","        ax = self.__arrange_graph(ax)\n","        ax.set_ylim([0.0, 0.25])\n","        ax.set_ylabel(\"SMoA\")\n","        ax.errorbar(self.iteration, self.mean_I_x, yerr=self.std_I_x\n","            , capsize=self.capsize, fmt='.-', markersize=self.markersize, ecolor='steelblue', markeredgecolor = \"steelblue\", color='steelblue', alpha=self.alpha, label=\"Ave:I_x\")\n","        ax.errorbar(self.iteration, self.mean_I_y, yerr=self.std_I_y\n","            , capsize=self.capsize, fmt='.:', markersize=self.markersize, ecolor='salmon', markeredgecolor = \"salmon\", color='salmon', alpha=self.alpha, label=\"Ave:I_y\")\n","        ax.errorbar(self.iteration, self.mean_I_r, yerr=self.std_I_r\n","            , capsize=self.capsize, fmt='.-.', markersize=self.markersize, ecolor='dimgray', markeredgecolor = \"dimgray\", color='dimgray', alpha=self.alpha, label=\"Ave:I_r\")\n","        ax.legend()\n","\n","        # FID\n","        ax = fig.add_subplot(6, 1, 5)\n","        ax.set_ylim([0.0, 200.0])\n","        ax = self.__arrange_graph(ax)\n","        ax.set_ylabel(\"FID\")\n","        ax.plot(self.iteration, self.FID_list, marker='.', linestyle=\"-\" , markersize=self.markersize, color='steelblue', label=\"FID\")\n","        ax.legend()\n","        \n","        \n","        # FID vs I_x, I_y, I_r\n","        ax = fig.add_subplot(6, 1, 6)\n","        ax = self.__arrange_graph(ax)\n","        ax.set_ylim([0.0, 0.003])\n","        ax.set_ylabel(\"I/FID\")\n","        ax.plot(self.iteration, self.mean_I_x/np.asarray(self.FID_list), marker='.', linestyle=\"-\" , markersize=self.markersize, color='steelblue', label=\"I_x/FID\")\n","        ax.plot(self.iteration, self.mean_I_y/np.asarray(self.FID_list), marker='.', linestyle=\":\" , markersize=self.markersize, color='salmon', label=\"I_y/FID\")\n","        ax.plot(self.iteration, self.mean_I_r/np.asarray(self.FID_list), marker='.', linestyle=\"-.\" , markersize=self.markersize, color='dimgray', label=\"I_r/FID\")\n","        ax.legend()\n","\n","        plt.savefig(f'{self.params.EXPERIMENTAL_NAME}-{self.iteration}.png') # -----(2)\n","        plt.show()\n","\n","    def __arrange_graph(self, ax):\n","        ax.set_xlabel('iteration(x1000)')\n","        ax.set_xlim(-0.5, 10+self.iteration[-1])\n","        ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(self.params.ITERATION_SPAN))\n","        return ax\n","\n","    def __initialize_losses(self):\n","\n","        self.__flush_losses()\n","        self.mean_acc_D_real, self.mean_acc_D_fake = [], []\n","        self.mean_loss_D_real, self.mean_loss_D_fake, self.mean_loss_G_fake = [], [], []\n","        self.mean_MSE_I_x, self.mean_MSE_I_y, self.mean_MSE_I_r = [], [], []\n","        self.mean_I_x, self.mean_I_y, self.mean_I_r = [], [], []\n","\n","        self.std_acc_D_real, self.std_acc_D_fake = [], []\n","        self.std_loss_D_real, self.std_loss_D_fake, self.std_loss_G_fake = [], [], []\n","        self.std_MSE_I_x, self.std_MSE_I_y, self.std_MSE_I_r = [], [], []\n","        self.std_I_x, self.std_I_y, self.std_I_r = [], [], []\n","\n","    def __flush_losses(self):\n","\n","        self.acc_D_real, self.acc_D_fake = [], []\n","        self.loss_D_real, self.loss_D_fake, self.loss_G_fake = [], [], []\n","        self.MSE_I_x, self.MSE_I_y, self.MSE_I_r = [], [], []\n","        self.I_x, self.I_y, self.I_r = [], [], []\n","    \n","    def __summarize_losses(self):\n","        self.mean_acc_D_real.append(np.mean(self.acc_D_real))\n","        self.std_acc_D_real.append(np.std(self.acc_D_real))\n","\n","        print(self.mean_acc_D_real, self.std_acc_D_real)\n","\n","        self.mean_acc_D_fake.append(np.mean(self.acc_D_fake))\n","        self.std_acc_D_fake.append(np.std(self.acc_D_fake))\n","\n","        self.mean_loss_D_real.append(np.mean(self.loss_D_real))\n","        self.std_loss_D_real.append(np.std(self.loss_D_real))\n","\n","        self.mean_loss_D_fake.append(np.mean(self.loss_D_fake))\n","        self.std_loss_D_fake.append(np.std(self.loss_D_fake))\n","\n","        self.mean_loss_G_fake.append(np.mean(self.loss_G_fake))\n","        self.std_loss_G_fake.append(np.std(self.loss_G_fake))\n","\n","        self.mean_MSE_I_x.append(np.mean(self.MSE_I_x))\n","        self.std_MSE_I_x.append(np.std(self.MSE_I_x))\n","\n","        self.mean_MSE_I_y.append(np.mean(self.MSE_I_y))\n","        self.std_MSE_I_y.append(np.std(self.MSE_I_y))\n","        \n","        self.mean_MSE_I_r.append(np.mean(self.MSE_I_r))\n","        self.std_MSE_I_r.append(np.std(self.MSE_I_r))\n","\n","        self.mean_I_x.append(np.mean(self.I_x))\n","        self.std_I_x.append(np.std(self.I_x))\n","\n","        self.mean_I_y.append(np.mean(self.I_y))\n","        self.std_I_y.append(np.std(self.I_y))\n","        \n","        self.mean_I_r.append(np.mean(self.I_r))\n","        self.std_I_r.append(np.std(self.I_r))\n","\n","        self.__flush_losses()\n","\n","    def calc_stat(self):\n","        return { \n","                      \"max_I_x\": np.max(self.mean_I_x)\n","                    , \"min_I_x\": np.min(self.mean_I_x)\n","                    , \"max_I_y\": np.max(self.mean_I_y)\n","                    , \"min_I_y\": np.min(self.mean_I_y)\n","                    , \"max_I_r\": np.max(self.mean_I_r)\n","                    , \"min_I_r\": np.min(self.mean_I_r)\n","\n","                    , \"mean_I_x\": np.mean(self.mean_I_x)\n","                    , \"mean_I_y\": np.mean(self.mean_I_y)\n","                    , \"mean_I_r\": np.mean(self.mean_I_r)\n","\n","                    , \"max_FID\": np.max(self.FID_list)\n","                    , \"min_FID\": np.min(self.FID_list)\n","\n","                    , \"max_I_x_FID\": np.max(np.asarray(self.mean_I_x)/np.asarray(self.FID_list))\n","                    , \"max_I_y_FID\": np.max(np.asarray(self.mean_I_x)/np.asarray(self.FID_list))\n","                    , \"max_I_r_FID\": np.max(np.asarray(self.mean_I_r)/np.asarray(self.FID_list))\n","\n","                    , \"min_I_x_FID\": np.min(np.asarray(self.mean_I_x)/np.asarray(self.FID_list))\n","                    , \"min_I_y_FID\": np.min(np.asarray(self.mean_I_y)/np.asarray(self.FID_list))\n","                    , \"min_I_r_FID\": np.min(np.asarray(self.mean_I_r)/np.asarray(self.FID_list))\n","                    } "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJPtKNhPqHE1","colab_type":"text"},"source":["# Train\n","\n","- MNIST: MNIST のデータを格納したクラス\n","- Params: パラメータを格納したクラス\n","\n","- trainer: 学習の本体"]},{"cell_type":"code","metadata":{"id":"Drv_yZu2hVIN","colab_type":"code","colab":{}},"source":["class MNIST:\n","    def __init__(self, params):\n","        \"\"\"\n","        今回は train_X のみ使用\n","        (test_X で断面二次モーメントの強さ比較してもいいかも)\n","        \"\"\"\n","        (train_X, _), (_, _) = tf.keras.datasets.mnist.load_data()\n","        train_X = train_X.astype(np.float32).reshape((-1, 28, 28, 1))/255.0\n","        self.train_X = train_X * 2.0 - 1.0 #range:[0.0, 1.0] -> [-1.0, 1.0]\n","\n","        #self.tfdata = tf.data.Dataset.from_tensor_slices(train_X).batch(params.BATCH_SIZE).map(self.rotate_tf).repeat(params.NUM_EPOCHS).shuffle(20)\n","        self.tfdata = tf.data.Dataset.from_tensor_slices(self.train_X).batch(params.BATCH_SIZE).repeat(params.NUM_EPOCHS).shuffle(20)\n","\n","class Params:\n","    def __init__(self):\n","        self.tf_version = tf.__version__\n","        self.sys_versionm = sys.version\n","\n","        self.ITERATION_SPAN = 4000\n","        self.NUM_EPOCHS = 20\n","        self.BATCH_SIZE  = 50\n","        self.MAX_ITERATION = 60000//self.BATCH_SIZE\n","\n","        self.NOISE_DIM   = 128\n","\n","        self.discriminator_loss = 'binary_crossentropy'\n","        self.discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","        self.discriminator_metrics = ['accuracy']\n","        \n","        self.generator_loss = {'possibility': 'binary_crossentropy', 'I_x': 'mean_squared_error', 'I_y': 'mean_squared_error', \"I_r\": \"mean_squared_error\"}\n","        self.generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","        self.generator_metrics = ['accuracy' , 'mean_squared_error', MetricsAverageSecondMomentOfArea()]\n","\n","        self.generator_loss_weights = {\"possibility\": 1.0, \"I_x\": 0.0, \"I_y\": 0.0, \"I_r\": 0.0} \n","\n","        EXP_TYPE = \"GAN\"\n","        self.EXPERIMENTAL_NAME = f\"{EXP_TYPE}_{(datetime.datetime.utcnow() + datetime.timedelta(hours=9)).strftime('%Y-%m-%d %H:%M:%S')}\"\n","        self.save_params() # ノイズ行列とか入ると視認性が下がるのでここで保存しておく\n","\n","        \"\"\"\n","        何回も使う行列はここで定義しておく\n","        \"\"\"\n","        self.REAL = np.ones(shape=(self.BATCH_SIZE, 1))\n","        self.FAKE = np.zeros(shape=(self.BATCH_SIZE, 1))\n","        self.FIXED_NOISE_FOR_PREDICT = np.random.normal(0, 1, (self.BATCH_SIZE, self.NOISE_DIM))\n","        # [1]xBATCHSIZE, [0]xBATCHSIZE ベクトル を使い回すことで、SMoAロスに対する正解データとする\n","        self.opt_I_x = [self.REAL, self.FAKE][0] \n","        self.opt_I_y = [self.REAL, self.FAKE][0]\n","        self.opt_I_r = [self.REAL, self.FAKE][0]\n","\n","    def save_params(self):\n","        with open(f'_{self.EXPERIMENTAL_NAME}_params.md', 'w') as f:\n","            f.write(f\"# Exp: {self.EXPERIMENTAL_NAME}\\n\")\n","            f.write(\"## params\\n\")\n","            for k, v in zip(self.__dict__.keys(), self.__dict__.values()):\n","                f.write(str(f\"{k}, {v}\")+\"\\n\")\n","\n","class Trainer:\n","    def __init__(self, params):\n","        self.params = params\n","        self.MNIST = MNIST(self.params)\n","        self.smoa = SecondMomentOfArea()\n","        self.GAN = GAN(self.params, self.smoa)\n","        self.Loss = Loss(self.params)\n","\n","        # Visualize layers.\n","        tf.keras.utils.plot_model(self.GAN.generator, to_file='_model_generator.png', show_shapes=True)\n","        tf.keras.utils.plot_model(self.GAN.discriminator, to_file='_model_discriminator.png', show_shapes=True)\n","        self.GAN.combined._layers = [ # rename\n","            layer for layer in self.GAN.combined._layers if isinstance(layer, tf.keras.layers.Layer)\n","          ]\n","        tf.keras.utils.plot_model(self.GAN.combined, to_file=\"_model_combined.png\", show_shapes=True)\n","\n","    def train(self):\n","        print(\"[start training]\")\n","\n","        for iteration, batch in enumerate(self.MNIST.tfdata, start=1):\n","\n","            y_real = batch\n","            z_in = np.random.normal(0, 1, (self.params.BATCH_SIZE, self.params.NOISE_DIM))\n","\n","            y_gen, I_x, I_y, I_r = self.GAN.generator.predict(z_in)\n","\n","            D_loss_real_dict = self.GAN.discriminator.train_on_batch(y_real, self.params.REAL, return_dict=True)\n","            D_loss_fake_dict = self.GAN.discriminator.train_on_batch(y_gen, self.params.FAKE, return_dict=True)\n","\n","            # Train Generator\n","            G_loss_dict = self.GAN.combined.train_on_batch(z_in, [self.params.REAL, self.params.opt_I_x, self.params.opt_I_y, self.params.opt_I_r], return_dict=True)\n","\n","            # Dump loss\n","            self.Loss.dump_loss_dicts(D_loss_real_dict, D_loss_fake_dict, G_loss_dict)\n","\n","            if(iteration%self.params.ITERATION_SPAN == 0): \n","                print(f\"iteration: {iteration}\")\n","                self.GAN.generator.save(f'model-{self.params.EXPERIMENTAL_NAME}-{iteration}.hdf5')\n","                generate_samples(params=self.params, GAN=self.GAN, iteration=iteration)\n","                FID = calc_fid_for_generator(params=self.params, GAN=self.GAN, MNIST=self.MNIST)\n","                self.Loss.dump_FID(FID) # Dump FID\n","                self.Loss.show_loss(iteration=iteration)\n","        return self.Loss.calc_stat()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VEBwODklMpp","colab_type":"text"},"source":["# 学習"]},{"cell_type":"code","metadata":{"id":"jql9QRKhvkuO","colab_type":"code","colab":{}},"source":["trainer = Trainer(Params()).train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vaKAwqQbk6Nw","colab_type":"text"},"source":["# パラメータサーチ用\n","\n","外側から Params をいじる\n","\n","- `NAME`: 各実験の保存用の名前を書く場所。自動にしてもよかった\n","- `loss_weights`: これがロスの配分になる\n","-  `power_I_x` / `power_I_y` / `power_I_r`: `0`は断面二次モーメントが増すような学習、`1` は下がるような学習になる"]},{"cell_type":"code","metadata":{"id":"5kxo1ph1xnRE","colab_type":"code","colab":{}},"source":["NAME = [\"Vanilla\", \"I_x+75\", \"I_y+75\", \"I_r+75\", \"I_x-500\", \"I_y-500\", \"I_r-500\"]\n","\n","for i, name in enumerate(NAME):\n","    print(f\"{i}, {name}\")\n","    loss_weights = [\n","        {\"possibility\": 1.0, \"I_x\": 0.0, \"I_y\": 0.0, \"I_r\": 0.0} #vanilla\n","      , {\"possibility\": 1.0, \"I_x\": 75.0, \"I_y\": 0.0, \"I_r\": 0.0} #I_x +75\n","      , {\"possibility\": 1.0, \"I_x\": 0.0, \"I_y\": 75.0, \"I_r\": 0.0} # I_y+75\n","      , {\"possibility\": 1.0, \"I_x\": 0.0, \"I_y\": 0.0, \"I_r\": 75.0}  # I_r + 75\n","      , {\"possibility\": 1.0, \"I_x\": 500.0, \"I_y\": 0.0, \"I_r\": 00.0} # I_x-500\n","      , {\"possibility\": 1.0, \"I_x\": 0.0, \"I_y\": 500.0, \"I_r\": 00.0} #I_y-500\n","      , {\"possibility\": 1.0, \"I_x\": 0.0, \"I_y\": 0.0, \"I_r\": 500.0}  #I_r-500\n","      , {\"possibility\": 1.0, \"I_x\": 50.0, \"I_y\": 50.0, \"I_r\": 0.0} # X+50 y-50\n","      , {\"possibility\": 1.0, \"I_x\": 50.0, \"I_y\": 50.0, \"I_r\": 0.0} # X-50 y-50\n","    ]\n","    power_I_x = [0, 0, 0, 0, 1, 0, 0, 0, 1]\n","    power_I_y = [0, 0, 0, 0, 0, 1, 0, 1, 0]\n","    power_I_r = [0, 0, 0, 0, 0, 0, 1, 0, 0]\n","\n","    class EXP1:\n","        def __init__(self, NAME):\n","            params = Params()\n","            params.EXPERIMENTAL_NAME = name\n","            params.generator_loss_weights = loss_weights[i]\n","            params.opt_I_x = [params.REAL, params.FAKE][power_I_x[i]] \n","            params.opt_I_y = [params.REAL, params.FAKE][power_I_y[i]]\n","            params.opt_I_r = [params.REAL, params.FAKE][power_I_r[i]]\n","            params.save_params()\n","            trainer = Trainer(params)\n","            self.statdict = trainer.train()\n","            self.save_params(params.EXPERIMENTAL_NAME)\n","                        \n","        def save_params(self, EXPERIMENTAL_NAME):\n","            with open(f'results_{EXPERIMENTAL_NAME}_params.md', 'w') as f:\n","                f.write(f\"# Exp: {EXPERIMENTAL_NAME}\\n\")\n","                f.write(\"## params\\n\")\n","                for k, v in zip(self.statdict.keys(), self.statdict.values()):\n","                    f.write(str(f\"{k}, {v}\")+\"\\n\")\n","    exp1 = EXP1(name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsscRxrqpHDt","colab_type":"text"},"source":["以上"]}]}